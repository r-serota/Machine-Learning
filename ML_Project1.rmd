---
title: "Final project - Big Data"
author: "Anna Kołacz, Reneeleonette Serota, Mariusz Zakrzewski, Kamil Stec"
date: "December 2019"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
---

```{r echo=F, warning=F, message=F}
# Libraries
  library(rmarkdown)
  library(dplyr)
  library(tidyr)
  library(leaps)
  library(e1071)
  library(ROCR)
  library(caTools)
  library(gbm)
  library(randomForest)
```

# Abstract

This work focuses on data gathered in a survey of student in portugese class. The dataset contains information about students and their performance in school. Our work tests main predictors of students grade. As a part of our experiments, classification methods were used to predict given information about student if he or she will pass the exam. Also, we used regression tools to predict number of points student will gather.

We begin by describing in-depth our dataset. Later, we clean the dataset. We remove some variables and justify our decision. Also students were divided by their final grade to those who passed their class on did not. The division was made by passing score equal to 60% of available points.

In the next chapter we used classification methods to predict whether or not student will pass the exam. For this task we used logistic regression, support vector machines, KNN and random forest. 

In third chapter regression methods are used to predict number of points student will gather on their final exam. 

# Dataset:

The data were obtained in a survey of students math and portuguese language courses in secondary school. It contains a lot of interesting social, gender and study information about students. 
Our project consists of predicting student's performance. Predicting results in mathematics and portugese without decoupling them seemed irrational. Therefore we had to choose student performance either in portugese or in mathematics. We decided to exclude survey conducted in mathematics, and focus only on portugese classes.

Experiments conducted in this document use following attributes:

* `school`: student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)
* `sex`: tudent's sex (binary: "F" - female or "M" - male)
* `age`: student's age (numeric: from 15 to 22)
* `address`: student's home address type (binary: "U" - urban or "R" - rural)
* `famsize`: family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)
* `Pstatus`: parent's cohabitation status (binary: "T" - living together or "A" - apart)
* `Medu`: mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
* `Fedu`: father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
* `reason`: reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")
* `studytime`: weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
* `failures`: number of past class failures (numeric: n if 1<=n<3, else 4)
* `schoolsup`: extra educational support (binary: yes or no)
* `famsup`: family educational support (binary: yes or no)
* `paid`: extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
* `activities`: extra-curricular activities (binary: yes or no)
* `nursery`: attended nursery school (binary: yes or no)
* `higher`: wants to take higher education (binary: yes or no)
* `internet`: Internet access at home (binary: yes or no)
* `famrel`: quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
* `freetime`: free time after school (numeric: from 1 - very low to 5 - very high)
* `Dalc`: workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `Walc`: weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `health`: current health status (numeric: from 1 - very bad to 5 - very good)
* `absences`: number of school absences (numeric: from 0 to 93)

Some of the attributes where excluded:

* `Mjob`: mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
* `Fjob`: father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
* `guardian`: student's guardian (nominal: "mother", "father" or "other")
* `romantic`: with a romantic relationship (binary: yes or no)
* `traveltime`: home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
* `goout`: going out with friends (numeric: from 1 - very low to 5 - very high)

Those variables seem to be rather irrelative with respect to predicting student performances, therefore they were excluded from the model.

## Cleaning data

At first, we have to load our dataset into memory. 
```{r warning=F, message=F}

# Reading the file and cleaning data
data <- read.table("student-por.csv", sep=";", header=TRUE)
data <- na.omit(data)

```

Later, we remove not important variables.
```{r warning=F, message=F}
# dropping variables
data <- subset(data, select = -c(G1, G2, school, Mjob, Fjob, guardian, traveltime, romantic, goout))
```

Variables were removed because of their insiginificance to student performance on their final exam. We belived that those features would cause the model to overfit and thus perform worse on test dataset. Variables G1 and G2 (first and second exam results) were excluded because we want to simulate situation where student has not yet taken any exam, and we want to predict if he will pass or not. 

Students are devided into those who passed the exam and who did not. The decision was made by passing score of 60%. Maximum number of score on that exam was 20, therefore passing score equals to 12.
```{r warning=F, message=F}
# adding the dependent variable
data$studentPerformance <- ifelse(data$G3 <= 12, "Bad", "Good")
```

```{r warning=F, message=F, echo=F}
VariablesClasses <- data %>% summarise_all(class) %>% gather # checking variables' classes
```

Clean the data - make sure appropriate variables are treated as factorial and ordered values.
```{r warning=F, message=F}
# changing type of variables
data$Medu <- factor(data$Medu, order=T, levels = c(0,1,2,3,4))
data$Fedu <- factor(data$Fedu, order=T, levels = c(0,1,2,3,4))
data$studytime <- factor(data$studytime, order=T, levels = c(1,2,3,4))
data$famrel <- factor(data$famrel, order=T, levels = c(1,2,3,4,5))
data$freetime <- factor(data$freetime, order=T, levels = c(1,2,3,4,5))
data$Dalc <- factor(data$Dalc, order=T, levels = c(1,2,3,4,5))
data$Walc <- factor(data$Walc, order=T, levels = c(1,2,3,4,5))
data$health <- factor(data$health, order=T, levels = c(1,2,3,4,5))
data$studentPerformance <- as.factor(data$studentPerformance)
```

## Descriptive analysis

We begin by describing our dataset more in-depth. 

```{r echo = FALSE, warning=FALSE, message=F}
summary(data)
hist(data$G3, breaks=0:20)
pie(table(data$studentPerformance)) # TODO: change to ggplot2
```

Histogram of students point in the final exam looks like a Gaussian bell curve.

# Classification

## Model selection

## Logistic regression
```{r }
dataLogit <- subset(data, select = -c(G3))

set.seed(1011)
labelsLogit = sample.split(dataLogit$studentPerformance, SplitRatio = 4/5) # We split the data in the proportion 80%/20%

# checking if the split is balanced
dataLogit_train = dataLogit[labelsLogit, ]
dataLogit_trainMean <- mean(as.numeric(dataLogit_train$studentPerformance)-1)
dataLogit_test = dataLogit[!labelsLogit, ]
dataLogit_testMean <- mean(as.numeric(dataLogit_test$studentPerformance)-1)

# fit the logistic regression model on the training sample only
glm.fit = glm(studentPerformance ~ ., data = dataLogit_train, family = binomial)

# predicted probabilities on the test sample 
glm.probs = predict(glm.fit, newdata = dataLogit_test, type = "response") 

# predicted signs on the test set
glm.pred = ifelse(glm.probs > 0.5, 1, 0)

# test sample confusion matrix
LogitConfMat <- table(true = dataLogit_test$studentPerformance, predict = glm.pred)

# test sample total success rate
dataLogit_test$studentPerformance <- ifelse(dataLogit_test$studentPerformance=="Good",1,0)
LogitSuccessRate <- mean(glm.pred == dataLogit_test$studentPerformance)
## TODO: upewnic sie, ze 1 oznacza zawsze Good

# test set ROC curve and AUC
predob = prediction(glm.probs, dataLogit_test$studentPerformance)
perf = performance(predob, "tpr", "fpr")
plot(perf, main = "Logistic Regression", colorize = TRUE)
LogitAUC <- as.numeric(performance(predob, "auc")@y.values)

# total test set success rate of the baseline model (with prediction 1 for every observation)
Basemodel <- mean(1 == dataLogit_test$studentPerformance)
```

## Support Vector Machine

```{r }

dataSVM <- subset(data, select = -c(G3))

# dividing our dataset into training and test sets (80%/20%)
train = sample(nrow(dataSVM), 0.8*nrow(dataSVM))
dataSVM_train = dataSVM[train, ]
dataSVM_test = dataSVM[-train, ]

dataSVM_test.response <- dataSVM_test$studentPerformance

### 10-CV using `tune()`
set.seed(1)
# cross-validating the tuning parameters gamma and lambda for the radial SVM om training data
tune.out = tune(svm, factor(studentPerformance) ~ ., data = dataSVM_train, kernel = "radial",
                ranges = list(cost = c(1e-10, 1e-5, 0.1 ,1 ,10 ,100 ,1000), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)

# retrieve the best model
svmfit.opt = tune.out$best.model
summary(svmfit.opt)

# confusion matrix on the test set
table(true = dataSVM_test.response,  pred = predict(svmfit.opt, dataSVM_test))

SVMErrorRate <- (1 - mean(predict(svmfit.opt, dataSVM_test) == dataSVM_test.response)) # error rate

# TODO: in our dataset Number of Support Vectors is very large. How should we pick C to change it?
# TODO: how to pick kernel, cost, gamma
# TODO: add ROC curve
# TODO: parameter tuning with library(kernlab)
# TODO: why can I not pick another proportion of training/test set???
# TODO: probably because our dataset contains a lot of categorical variables, SVM method behavies very poorly. We need to verify it, and write about it in out report

```

## K-Nearest Neigbours

## Linear Discriminant Analysis

## Random Forests, Boosting, Bagging

```{r}

## RF

dataRF <- subset(data, select = -c(G3))

labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
dataRF_train = dataRF[labelsRF,]

# checking if 500 trees is enough for our dataset
checking = randomForest(studentPerformance ~ ., data = dataRF_train)
checkigPlot <- plot(checking$err.rate[,1]) # Yes

auc_value20 = 0.0
repeat_each = 20
for (i in 1:repeat_each) {
  
  labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
  dataRF_train = dataRF[labelsRF,]
  dataRF_test = dataRF[!labelsRF,]
 
  randomForestResult = randomForest(studentPerformance ~ ., data = dataRF_train, mtry = 4, ntree = 500) 
  # We use m = 4, because sqrt(23) = 4.8, p = 23
  
  rf.probs = predict(randomForestResult, dataRF_test, type = "prob")[, 2]
  predob.rf = prediction(rf.probs, dataRF_test$studentPerformance)
  perf = performance(predob.rf, "tpr", "fpr")
  
  auc_value <- as.numeric(performance(predob.rf, "auc")@y.values)
  auc_value20 = auc_value20 + auc_value
}

mean_auc = auc_value20 / repeat_each

# variable importance
rf = randomForest(studentPerformance ~ ., data = dataRF_train, mtry = 4, ntree = 500)
varImpPlot(rf) # plot importance

# Plot for RF
par(mfrow = c(1, 2))
plot(perf, main = "Random Forest, m = 4", colorize = TRUE) 

# TODO: make sure if m = 23
# TODO: what about OOB?


### Bagging:
BaggingResult = randomForest(studentPerformance ~ ., data = dataRF_train, mtry = 23, ntree = 500)
bagg.probs = predict(BaggingResult, dataRF_test, type = "prob")[, 2]
predob.bagg = prediction(bagg.probs, dataRF_test$studentPerformance)
perf = performance(predob.bagg, "tpr", "fpr")
plot(perf, main = "Bagging, m = 23", colorize = TRUE)
par(mfrow = c(1, 1))

cat("RandomForest (m = 4): ", mean_auc)
cat("\n")
cat("Bagging      (m = 23): ", as.numeric(performance(predob.bagg, "auc")@y.values))


### Boosting

dataBoosting <- subset(data, select = -c(G3))
dataBoosting$studentPerformance <- as.integer(dataBoosting$studentPerformance) 

labelsBoosting = sample.split(dataBoosting$studentPerformance, SplitRatio = 4/5)
dataBoosting_train = dataBoosting[labelsBoosting,]
dataBoosting_test = dataBoosting[!labelsBoosting,]

# We use CV to pick the number of trees.
boostingCV = gbm((studentPerformance-1) ~ ., data = dataBoosting_train, distribution = 'bernoulli', n.trees = 10000, 
                 shrinkage = 0.01, cv.folds = 10)
# Bernoulli requires the response to be in {0,1}. That's why we use (studentPerformance-1)
best.iter = gbm.perf(boostingCV, method = "cv")# Check the best iteration
# We fit the optimal boosted tree.
boostingBest = gbm((studentPerformance-1) ~ ., data = dataBoosting_train, distribution = 'bernoulli', 
                   n.trees = best.iter, shrinkage = 0.01)

# test set ROC curve and the AUC for the best boosted tree.
boost.probs = predict(boostingBest, dataBoosting_test, n.trees = best.iter, type = "response")
predob = prediction(boost.probs, dataBoosting_test$studentPerformance)
perf = performance(predob, "tpr", "fpr")
plot(perf, main = "Boosting with B = 760", colorize = TRUE) #TODO: B
as.numeric(performance(predob, "auc")@y.values)

# Variable importance plot
summary(boostingBest)

#TODO: how to pick shrinkage, d, distribution


```

## Classification summary

# Regression

## Model selection

## Linear regression

## Random Forests

## Regression summary

# Summary










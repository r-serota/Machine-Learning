---
title: "Final project - Big Data"
author: "Anna Ko≈Çacz, Reneeleonette Serota, Mariusz Zakrzewski, Kamil Stec"
date: "December 2019"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
---

```{r echo=F, warning=F, message=F}
# Libraries
  library(rmarkdown)
  library(data.table)
  library(dplyr)
  library(tidyr)
  library(leaps)
  library(e1071)
  library(ROCR)
  library(caTools)
```

# Abstract

# Dataset:
* `gvkey`: firm identification number
...


## Cleaning data
```{r warning=F, message=F}

# Reading the file and cleaning data
data <- read.table("student-por.csv", sep=";", header=TRUE)
data <- na.omit(data)

# dropping variables
data <- subset(data, select = -c(G1, G2, school, Mjob, Fjob, guardian, traveltime, romantic, goout))

# adding the dependent variable
data$studentPerformance <- ifelse(data$G3 <= 10, "Bad", "Good")

VariablesClasses <- data %>% summarise_all(class) %>% gather # checking variables' classes

# changing type of variables
data$Medu <- factor(data$Medu, order=T, levels = c(0,1,2,3,4))
data$Fedu <- factor(data$Fedu, order=T, levels = c(0,1,2,3,4))
data$studytime <- factor(data$studytime, order=T, levels = c(1,2,3,4))
data$famrel <- factor(data$famrel, order=T, levels = c(1,2,3,4,5))
data$freetime <- factor(data$freetime, order=T, levels = c(1,2,3,4,5))
data$Dalc <- factor(data$Dalc, order=T, levels = c(1,2,3,4,5))
data$Walc <- factor(data$Walc, order=T, levels = c(1,2,3,4,5))
data$health <- factor(data$health, order=T, levels = c(1,2,3,4,5))
data$studentPerformance <- as.factor(data$studentPerformance)

```

## Descriptive analysis
```{r echo = FALSE, warning=FALSE, message=F}
summary(data)
hist(data$G3)
pie(table(data$studentPerformance)) # TODO: change to ggplot2

```


```{r echo = FALSE, warning=FALSE, message=F}
# only output
```

# Classification

## Model selection

## Logistic regression
```{r }
dataLogit <- subset(data, select = -c(G3))

set.seed(1011)
labelsLogit = sample.split(dataLogit$studentPerformance, SplitRatio = 4/5) # TODO: how large should be the SplitRatio ??? 

# checking if the split is balanced
dataLogit_train = dataLogit[labelsLogit, ]
dataLogit_trainMean <- mean(as.numeric(dataLogit_train$studentPerformance)-1)
dataLogit_test = dataLogit[!labelsLogit, ]
dataLogit_testMean <- mean(as.numeric(dataLogit_test$studentPerformance)-1)

# fit the logistic regression model on the training sample only
glm.fit = glm(studentPerformance ~ ., data = dataLogit_train, family = binomial)

# predicted probabilities on the test sample 
glm.probs = predict(glm.fit, newdata = dataLogit_test, type = "response") 

# predicted signs on the test set
glm.pred = ifelse(glm.probs > 0.5, 1, 0)

# test sample confusion matrix
LogitConfMat <- table(true = dataLogit_test$studentPerformance, predict = glm.pred)

# test sample total success rate
dataLogit_test$studentPerformance <- ifelse(dataLogit_test$studentPerformance=="Good",1,0)
LogitSuccessRate <- mean(glm.pred == dataLogit_test$studentPerformance)
## TODO: upewnic sie, ze 1 oznacza zawsze Good

# test set ROC curve and AUC
predob = prediction(glm.probs, dataLogit_test$studentPerformance)
perf = performance(predob, "tpr", "fpr")
plot(perf, main = "Logistic Regression", colorize = TRUE)
LogitAUC <- as.numeric(performance(predob, "auc")@y.values)

# what is the success rate on the test data of the baseline model?
# with prediction 1 for every observation, the  total test set success rate of the baseline model is
Basemodel <- mean(1 == dataLogit_test$studentPerformance)
```

## Support Vector Machine

```{r }

dataSVM <- subset(data, select = -c(G3))

# dividing our dataset into training and test sets (50%/50%)
train = sample(nrow(dataSVM), 0.8*nrow(dataSVM))
dataSVM_train = dataSVM[train, ]
dataSVM_test = dataSVM[-train, ]

dataSVM_test.response <- dataSVM_test$studentPerformance

### 10-CV using `tune()`
set.seed(1)
# cross-validating the tuning parameters gamma and lambda for the radial SVM om training data
tune.out = tune(svm, factor(studentPerformance) ~ ., data = dataSVM_train, kernel = "radial",
                ranges = list(cost = c(1e-10, 1e-5, 0.1 ,1 ,10 ,100 ,1000), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)

# retrieve the best model
svmfit.opt = tune.out$best.model
summary(svmfit.opt)

# confusion matrix on the test set
table(true = dataSVM_test.response,  pred = predict(svmfit.opt, dataSVM_test))

SVMErrorRate <- (1 - mean(predict(svmfit.opt, dataSVM_test) == dataSVM_test.response)) # error rate

# TODO: in our dataset Number of Support Vectors is very large. How should we pick C to change it?
# TODO: how to pick kernel, cost, gamma
# TODO: add ROC curve
# TODO: parameter tuning with library(kernlab)
# TODO: why can I not pick another proportion of training/test set???
# TODO: probably because our dataset contains a lot of categorical variables, SVM method behavies very poorly. We need to verify it, and write about it in out report

# Basic version
# svmfit = svm(factor(studentPerformance) ~ ., data = dataSVM_train, kernel = "radial",  gamma = 2, cost = 1)
# svm.pred <- predict(svmfit, newx = dataSVM[-train, ])
# table(dataSVM[-train, ]$studentPerformance, svm.pred)
# summary(svmfit)
```

## K-Nearest Neigbours

## Linear Discriminant Analysis

## Random Forests, Boosting, Bagging

## Classification summary

# Regression

## Model selection

## Linear regression

## Random Forests

## Regression summary

# Summary






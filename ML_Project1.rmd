---
title: "Final project - Big Data"
author: "Anna Kołacz, Reneeleonette Serota, Mariusz Zakrzewski, Kamil Stec"
date: "December 2019"
output:
  html_document
---

```{r echo=F, warning=F, message=F}
# Libraries
  library(rmarkdown)
  library(dplyr)
  library(tidyr)
  library(leaps)
  library(e1071)
  library(ROCR)
  library(caTools)
  library(gbm)
  library(randomForest)
  library(ggplot2)
```

```{r setup, include=F}
# setwd("C:/Users/anaik/Desktop/BigData/Project/student")
set.seed(1011) # set random number generator seed for replicability
```

# Abstract

This work focuses on data gathered in a survey of a secondary school's students. The dataset contains information about students  and their performance in school. Our work has two major goals: first to predict whether a student will pass an exam or not, second to predict the exact number of points student will score.

We begin by elaborating the used dataset as well as methods which we applied to clean the database. 
In the next chapter we implemented various classification methods to predict whether a student will have a good or bad exam's performance. For this task we used logistic regression, support vector machines, KNN and random forest. 
In the 3. chapter, we explored regression methods to predict the number of points students will score in their final exam. We implemented ... (TODO:which methods)
At the end, we summarised as well as compared obtained Machine Learning algorithms scores.


# Dataset

Data was obtained in a survey of a secondary school's students. It is available under the link [https://archive.ics.uci.edu/ml/datasets/student+performance](https://archive.ics.uci.edu/ml/datasets/student+performance).

The dataset contains variables about social, personal and study aspects as well as information about obtained number of points in Maths and Portugese language. Because of major part of the results in Portugese language and our reluctance to combine grades from entirely different courses, we concentrated on Portugese language's student in our research.  
What is more, we decided to exclude a few variables, because of a negligible relationship with the dependent variable from our project. Moreover, variables G1 and G2 (first and second exam results) were excluded because we want to simulate a situation where student has not taken any exam yet and predict if he will pass the exam or not.  
Additionaly, we created a variable 'studentPerformance' which divides the students into those who passed the exam and who did not. The division was made by passing score of 60%. Maximum number of score on that exam was 20, therefore passing score amounts to 12.

The following list shows the dependent variables used in the research:

* `school`: student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)
* `sex`: tudent's sex (binary: "F" - female or "M" - male)
* `age`: student's age (numeric: from 15 to 22)
* `address`: student's home address type (binary: "U" - urban or "R" - rural)
* `famsize`: family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)
* `Pstatus`: parent's cohabitation status (binary: "T" - living together or "A" - apart)
* `Medu`: mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
* `Fedu`: father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
* `reason`: reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")
* `studytime`: weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
* `failures`: number of past class failures (numeric: n if 1<=n<3, else 4)
* `schoolsup`: extra educational support (binary: yes or no)
* `famsup`: family educational support (binary: yes or no)
* `paid`: extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
* `activities`: extra-curricular activities (binary: yes or no)
* `nursery`: attended nursery school (binary: yes or no)
* `higher`: wants to take higher education (binary: yes or no)
* `internet`: Internet access at home (binary: yes or no)
* `famrel`: quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
* `freetime`: free time after school (numeric: from 1 - very low to 5 - very high)
* `Dalc`: workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `Walc`: weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `health`: current health status (numeric: from 1 - very bad to 5 - very good)
* `absences`: number of school absences (numeric: from 0 to 93)

The following list shows the atributes excluded from the research:

* `Mjob`: mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
* `Fjob`: father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
* `guardian`: student's guardian (nominal: "mother", "father" or "other")
* `romantic`: with a romantic relationship (binary: yes or no)
* `traveltime`: home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
* `goout`: going out with friends (numeric: from 1 - very low to 5 - very high)

  
In the classification task, G3 (final grade, numeric variable from 0 to 20) served as the dependent variable. 
In the regression task, the variable studentPerformance was our object of study.
final grade (numeric: from 0 to 20, output target)

```{r echo=F, warning=F, message=F}

# Reading the file and cleaning data
data <- read.table("student-por.csv", sep=";", header=TRUE)
data <- na.omit(data)

# adding the dependent variable
data$studentPerformance <- ifelse(data$G3 < 12, "Bad", "Good")

# dropping variables
data <- subset(data, select = -c(G1, G2, school, Mjob, Fjob, guardian, traveltime, romantic, goout))

# changing type of variables
data$Medu <- factor(data$Medu, order=T, levels = c(0,1,2,3,4))
data$Fedu <- factor(data$Fedu, order=T, levels = c(0,1,2,3,4))
data$studytime <- factor(data$studytime, order=T, levels = c(1,2,3,4))
data$famrel <- factor(data$famrel, order=T, levels = c(1,2,3,4,5))
data$freetime <- factor(data$freetime, order=T, levels = c(1,2,3,4,5))
data$Dalc <- factor(data$Dalc, order=T, levels = c(1,2,3,4,5))
data$Walc <- factor(data$Walc, order=T, levels = c(1,2,3,4,5))
data$health <- factor(data$health, order=T, levels = c(1,2,3,4,5))
data$studentPerformance <- as.factor(data$studentPerformance)

```


```{r echo = FALSE, warning=FALSE, message=F, fig.align="center", fig.width=3, fig.height=3}

hist(data$G3, breaks=0:20, main="G3 (final grade)")
abline(v = 12, col="blue", lwd=4, lty=1)

```

As can be seen on the histogram, there are few students with a number of points below 5 as well as few students with nearly maximum number of points. 
Most of learners obtained half of possible number of points.

The below plot represents a proportion of students whose passed and failed the exam.

```{r echo=F, fig.width=4, fig.height=4, fig.align="center"}
blank_theme <- theme_minimal()+   theme(axis.title.x = element_blank(), axis.title.y = element_blank(),
    panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=14, face="bold"))

data_pie <- table(data$studentPerformance)
data_pie <- as.data.frame(data_pie)
colnames(data_pie) <- c("group", "value")

ggplot(data_pie, aes(x="", y=value, fill=group)) + geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start=0) + blank_theme + theme(axis.text.x=element_blank()) +
  geom_text(aes(y = value/3 + c(0, cumsum(value)[-length(value)]), label = paste0(round((value/sum(data_pie$value)*100),2), "%")), size=5) + scale_fill_brewer("Blues") + 
  guides(fill=guide_legend("")) + ggtitle("Student performance")

```

As shown in the figure above, the proportion of passed and failed exams is nearly equal.

# Classification

## Model selection

## Logistic regression

In this chapter, some results concerning Logistic regression were presented.  

We implemented the Logistic Regression model and compared its result for training and test set. What is more, we calculated the Confusion Matrix (for threashold equal to $0.5$). In order to check the model's performance, we compared the obtained success rate of the test set with the base model which was built by a random guessing.


```{r fig.align="center", fig.width = 4, fig.height = 5 }
dataLogit <- subset(data, select = -c(G3))

# split the data in the proportion 80%/20%
labelsLogit = sample.split(dataLogit$studentPerformance, SplitRatio = 4/5) 

dataLogit_train = dataLogit[labelsLogit, ]
dataLogit_test = dataLogit[!labelsLogit, ]

```


```{r echo=F}
# check if the split is balanced
dataLogit_trainMean <- mean(as.numeric(dataLogit_train$studentPerformance)-1)
dataLogit_testMean <- mean(as.numeric(dataLogit_test$studentPerformance)-1)
```

```{r }
# fit the logistic regression model on the training sample only
glm.fit = glm(studentPerformance ~ ., data = dataLogit_train, family = binomial)
# predicted probabilities on the test sample 
glm.probs = predict(glm.fit, newdata = dataLogit_test, type = "response") 
# predicted signs on the test set (for threshold equal to 0.5)
glm.pred = ifelse(glm.probs > 0.5, 1, 0)

# Confusion matrix and SuccessRate on training set
glm.probs_train = predict(glm.fit, newdata = dataLogit_train, type = "response") 
glm.pred_train = ifelse(glm.probs_train > 0.5, 1, 0)
LogitConfMat_train <- table(true = dataLogit_train$studentPerformance, predict = glm.pred_train)
dataLogit_train$studentPerformance <- ifelse(dataLogit_train$studentPerformance=="Good",1,0)
LogitSuccessRate_train <- mean(glm.pred_train == dataLogit_train$studentPerformance)
LogitSuccessRate_train
```
On the training test, the Logistic regression model achieved the success rate equal to 81.92%. However, because of frequent occurance of overfitting, it is better to compare algorithms' scores on the test set.
```{r }
# Confusion matrix on the test set
LogitConfMat <- table(true = dataLogit_test$studentPerformance, predict = glm.pred)
LogitConfMat
```
Taking into account the test set, the model predicted correctly 17 students with Bad Performance and 80 students with Good Performance.
```{r }
# test sample total success rate
dataLogit_test$studentPerformance <- ifelse(dataLogit_test$studentPerformance=="Good",1,0)
mean(glm.pred == dataLogit_test$studentPerformance) # logistic regression success rate
```
It results in the correct prediction at the level of 75.19%.
```{r }
# total test set success rate of the baseline model (with prediction 1 for every observation)
Basemodel <- mean(1 == dataLogit_test$studentPerformance)
```
In order to determine if the obtained statistics is good or not, we compare the obtained result with the base model which is a reflection of a random guessing. Its success rate is equal to 69,77%. It follows that the logistic regression model gives us an improvement of the success rate at the level of 5.4 percentage points.
```{r fig.width=4, fig.height=4, fig.align="center"}
# test set ROC curve and AUC
predob = prediction(glm.probs, dataLogit_test$studentPerformance)
perf = performance(predob, "tpr", "fpr")
plot(perf, main = "Logistic Regression", colorize = TRUE, 
     print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7))
as.numeric(performance(predob, "auc")@y.values) # Logistic regression AUC score
```
As can be seen on the ROC curve, there is a large advantage of the true positive rate for a threshold's range of 0.0 - 0.7. 
...
The AUC for the logistic regression amounts to 78.86%.


## Support Vector Machines

In this chapter we use SVM for predicting if student will pass the final exam.  
Firstly, we used 10-CV to find tuning parameters $\gamma$ and $\lambda$ (equal to $1/C$, $C$ is the cost of a violation to the margin). As a kernel the "radial" function was used.
After picking the best tuning parameters, we built the model with them and measured the performance of SVM algorithm.

```{r }

dataSVM <- subset(data, select = -c(G3))

# dividing our dataset into training and test sets (80%/20%)
train = sample(nrow(dataSVM), 0.8*nrow(dataSVM))
dataSVM_train = dataSVM[train, ]
dataSVM_test = dataSVM[-train, ]

dataSVM_test.response <- dataSVM_test$studentPerformance

### 10-CV using `tune()`
# cross-validating the tuning parameters gamma and lambda for the radial SVM om training data
tune.out = tune(svm, factor(studentPerformance) ~ ., data = dataSVM_train, kernel = "radial",
                ranges = list(cost = c(1e-10, 1e-5, 0.1 ,1 ,10 ,100 ,1000), gamma = c(0.5, 1, 2, 3, 4)))

# retrieve the best model
svmfit.opt = tune.out$best.model
summary(svmfit.opt)
```
As we can see in the results, the number of Support Vectors in our experiment is equal to 519 (much more than the $p=23$). We assume a reason for such a large number can be the fact that the major part of predictors in our dataset are factorial and many of them are also binary.  

SVM is not a probability classifier. The result of its prediction is simply a binary response. Therefore we cannot use ROC curve and AUC metric to measure its accuracy and compare it to other classification algorithms. That is why we used an accuracy metric error rate to check the SVM performance. What is more, we calculated also the confusion matrix. 
```{r }
# confusion matrix on the test set
table(true = dataSVM_test.response,  pred = predict(svmfit.opt, dataSVM_test))
mean(predict(svmfit.opt, dataSVM_test) != dataSVM_test.response) # error rate
```
The error rate of SVM classifier is equal to $0.2692308$.

## K-Nearest Neigbours

## Linear Discriminant Analysis

## Random Forests and Bagging

This chapter concentrates on predicting the student perfomance using Random Forests and Bagging.  
At the beginning, we checked wheter the default value for tuning parameter of Random Forests (500 decision trees in a model) is proper for our dataset. 
Later, we compared performance of Random Forests and Bagging method by measuring averaged AUC of models. Moreover, we created Importance Plots for applied algorithms.

In order to check the tuning parameter we plottedthe OOB error.
```{r}
dataRF <- subset(data, select = -c(G3))

labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
dataRF_train = dataRF[labelsRF,]

# checking if 500 trees is enough for our dataset
checking = randomForest(studentPerformance ~ ., data = dataRF_train)
plot(checking$err.rate[,1], xlab="Number of trees", ylab="OOB error rate")  
```

The OOB error rate decreases significantly to around 50 trees. For a larger number of trees the difference of the OOB error is slight. Therefore we conclude that $B=500$ is sufficient for our dataset.

We built multiple models of Random Forests, tested its accuracy and averaged the results for obtaining a better estimation. Accuracy of model was measured by AUC (Area Under the Curve). While applying Random Forest we used default tuning parameters - $500$ trees and $\lfloor sqrt(p)\rfloor = 4$ (for classification).
```{r}
auc_value20 = 0.0
repeat_each = 20
for (i in 1:repeat_each) {
  
  labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
  dataRF_train = dataRF[labelsRF,]
  dataRF_test = dataRF[!labelsRF,]
 
  randomForestResult = randomForest(studentPerformance ~ ., data = dataRF_train)

  rf.probs = predict(randomForestResult, dataRF_test, type = "prob")[, 2]
  predob.rf = prediction(rf.probs, dataRF_test$studentPerformance)
  perf = performance(predob.rf, "tpr", "fpr")
  
  auc_value <- as.numeric(performance(predob.rf, "auc")@y.values)
  auc_value20 = auc_value20 + auc_value
}
auc_value20 / repeat_each # mean auc
```
The accuracy of Classification Random Forest measured by AUC is equal to $0.8011636$. 

Let us check importance of each variable with respect to predict studentPerformance.
```{r }
rf = randomForest(studentPerformance ~ ., data = dataRF_train)
varImpPlot(rf) # plot importance
```

The importance of variables was measured for each tree by the improvement in the split-criterion. As a metric of improvement in split-criterion, Gini Index was served.   
On the above plot we can see that the most important predictors of students success are: number of their absences, mother education, measure of health, reason for school choice and study time.

Below, we plotted the ROC curve for Random Forest with default $mtry$ and with $mtry=p$ which is equivalent to Bagging method. 
```{r }
BaggingResult = randomForest(studentPerformance ~ ., data = dataRF_train, mtry = 23, ntree = 500)
bagg.probs = predict(BaggingResult, dataRF_test, type = "prob")[, 2]
predob.bagg = prediction(bagg.probs, dataRF_test$studentPerformance)
perf.bagg = performance(predob.bagg, "tpr", "fpr")

par(mfrow = c(1, 2))
plot(perf, main = "Random Forest, m = 4", colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7)) 
plot(perf.bagg, main = "Bagging, m = 23", colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7))
```

As can be seen on the above plot, the ROC curve for the Random Forest indicated a better accuracy.   Nevertheless, in order to achieve more reliable comparison, we compared averaged accuracy (measured by AUC) for these methods. Both errors were estimated on the test data.
```{r echo=F, message=F}
par(mfrow = c(1, 1)) # fix the plot for later usage
```

```{r }
auc_value20 = 0.0
repeat_each = 20

for (i in 1:repeat_each) {
  labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
  dataRF_train = dataRF[labelsRF,]
  dataRF_test = dataRF[!labelsRF,]
 
  BaggingResult = randomForest(studentPerformance ~ ., data = dataRF_train, mtry = 23, ntree = 500)
  bagg.probs = predict(BaggingResult, dataRF_test, type = "prob")[, 2]
  predob.bagg = prediction(bagg.probs, dataRF_test$studentPerformance)
  
  auc_value <- as.numeric(performance(predob.bagg, "auc")@y.values)
  auc_value20 = auc_value20 + auc_value
}

cat("RandomForest (m = 4): ", auc_value20 / repeat_each, 
    "\nBagging      (m = 23): ", as.numeric(performance(predob.bagg, "auc")@y.values))
```

For Random Forest ($m = 4$) AUC curve is equal to $0.8011636$.
For Bagging ($m = 23$) AUC curve is equal to $0.7941455$. 
By applying Bagging method we see a slight deterioration of the model accuracy.


### Boosting

This chapter focuses on prediction the student performance by using Boosting method.

At the beginning, we used Cross-Validation which enabled us to pick a proper number of trees equal to ???. According to this value, we built a Boosting model, plotted the ROC curve, calucalted the AUC and presented which variables were considered as the most important.

```{r }
dataBoosting <- subset(data, select = -c(G3))
dataBoosting$studentPerformance <- as.integer(dataBoosting$studentPerformance) 

labelsBoosting = sample.split(dataBoosting$studentPerformance, SplitRatio = 4/5)
dataBoosting_train = dataBoosting[labelsBoosting,]
dataBoosting_test = dataBoosting[!labelsBoosting,]

# We use CV to pick the number of trees.
boostingCV = gbm((studentPerformance-1) ~ ., data = dataBoosting_train, distribution = 'bernoulli', n.trees = 10000, 
                 shrinkage = 0.01, cv.folds = 10)
# Bernoulli requires the response to be in {0,1}. That's why we use (studentPerformance-1)
best.iter = gbm.perf(boostingCV, method = "cv", plot.it = F)# Check the best iteration
# We fit the optimal boosted tree.
boostingBest = gbm((studentPerformance-1) ~ ., data = dataBoosting_train, distribution = 'bernoulli', 
                   n.trees = best.iter, shrinkage = 0.01)

# test set ROC curve and the AUC for the best boosted tree.
boost.probs = predict(boostingBest, dataBoosting_test, n.trees = best.iter, type = "response")
predob = prediction(boost.probs, dataBoosting_test$studentPerformance)
perf = performance(predob, "tpr", "fpr")
plot(perf, main = "Boosting with B = 760", colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7))
as.numeric(performance(predob, "auc")@y.values)
```
For Boosting method (with $B = 760$) the AUC metric is equal to $0.7820606$.
```{r }
# Variable importance plot
summary(boostingBest)
```

According to the obtained results of Boosting, the most relative predictor is 'number of failures'. A the next important variables the Boosting Algorithm indicated 'mother education', 'number of absences', 'reason to choose particular school'.


## Classification summary

For the first task, predicting if a student will pass the exam, 6 methods were used: Logistic Regression, KNN, Linear Description Analysis, Support Vector Machines, Random Forest, Bagging and Boosting Every method has been tested on validation test, which was randomly selected in proportion of 80/20. The obtained results are presented in the table below. 

Algorithm | AUC | Error rate
------------- | ------------- | ------------- 
Logistic regression | 0.7886 | TODO
SVM | - | 0.27 |
KNN | TODO | TODO 
LDA | TODO | TODO 
Random forest | 0.80 | 
Bagging | TODO | TODO 
Boosting | 0.78 | TODO 

TODO: opisanie co wygrywa.

Moreover, for Random Forest, Bagging and Boosting we conducted analysis of Variable Importance. The most feequent occuring variables were:  number of absences, mothers education, student health (measured by 1-5 scale), reason to choose particular school, weekly study time.

# Regression

## Model selection

## Linear regression

## Random Forests

## ...

## Regression summary









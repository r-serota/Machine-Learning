---
title: "Final project - Big Data"
author: "Anna Kołacz, Reneeleonette Serota, Mariusz Zakrzewski, Kamil Stec"
date: "December 2019"
output: html_document
---

```{r echo=F, warning=F, message=F}
# Libraries
  library(rmarkdown)
  library(dplyr)
  library(tidyr)
  library(leaps)
  library(e1071)
  library(ROCR)
  library(caTools)
  library(gbm)
  library(randomForest)
```

# Abstract

This work focuses on data gathered in a survey of a secondary school's students. The dataset contains information about students and their performance in school. Our work has two major goaals: first to predict whether a student will pass an exam or not, secondary to predict the exact number of points student will receive/achieve(?).

We begin by elaborating the used dataset as well as methods which we applied to clean the database. 

In the next chapter we implemented various classification methods to predict whether a student will have a good or bad exam's performance. For this task we used logistic regression, support vector machines, KNN and random forest. 

In the 3. chapter, we explored regression methods to predict the number of points students will achieve(?) in their final exam. We implemented ... (TODO:which methods)

At the end, we summarised as well as compared obtained Machine Learning algorithms scores.


# Dataset:

Data was obtained in a survey of a secondary school's students. It is available under the link https://www.kaggle.com/uciml/student-alcohol-consumption. The dataset contains variables about social, study, ... aspects as well as information about obtained number of points in Maths and Portugese language. Because of major part of the results in Portugese language and our reluctance to combine grades from entirely different courses, we concentrated on Portugese language's student in our research. What is more, we decided to exclude a few variables, because of a negligible relationship with the dependent variable from our project. Moreover, variables G1 and G2 (first and second exam results) were excluded because we want to simulate a situation where student has not taken any exam yet and then predict if he will pass or not. 
Additionaly, we created a variable 'Student_performance' which divides the students into those who passed the exam and who did not. The division was made by passing score of 60%. Maximum number of score on that exam was 20, therefore passing score amounts to 12.

The following list shows the atributes used in the research:

* `school`: student's school (binary: "GP" - Gabriel Pereira or "MS" - Mousinho da Silveira)
* `sex`: tudent's sex (binary: "F" - female or "M" - male)
* `age`: student's age (numeric: from 15 to 22)
* `address`: student's home address type (binary: "U" - urban or "R" - rural)
* `famsize`: family size (binary: "LE3" - less or equal to 3 or "GT3" - greater than 3)
* `Pstatus`: parent's cohabitation status (binary: "T" - living together or "A" - apart)
* `Medu`: mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
* `Fedu`: father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
* `reason`: reason to choose this school (nominal: close to "home", school "reputation", "course" preference or "other")
* `studytime`: weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
* `failures`: number of past class failures (numeric: n if 1<=n<3, else 4)
* `schoolsup`: extra educational support (binary: yes or no)
* `famsup`: family educational support (binary: yes or no)
* `paid`: extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
* `activities`: extra-curricular activities (binary: yes or no)
* `nursery`: attended nursery school (binary: yes or no)
* `higher`: wants to take higher education (binary: yes or no)
* `internet`: Internet access at home (binary: yes or no)
* `famrel`: quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
* `freetime`: free time after school (numeric: from 1 - very low to 5 - very high)
* `Dalc`: workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `Walc`: weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `health`: current health status (numeric: from 1 - very bad to 5 - very good)
* `absences`: number of school absences (numeric: from 0 to 93)

The following list shows the atributes excluded from the research:

* `Mjob`: mother's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
* `Fjob`: father's job (nominal: "teacher", "health" care related, civil "services" (e.g. administrative or police), "at_home" or "other")
* `guardian`: student's guardian (nominal: "mother", "father" or "other")
* `romantic`: with a romantic relationship (binary: yes or no)
* `traveltime`: home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
* `goout`: going out with friends (numeric: from 1 - very low to 5 - very high)

... 


## Cleaning data
```{r warning=F, message=F}

# Reading the file and cleaning data
setwd("C:/Users/anaik/Desktop/BigData/Project/student")
data <- read.table("student-por.csv", sep=";", header=TRUE)
data <- na.omit(data)

# adding the dependent variable
data$studentPerformance <- ifelse(data$G3 < 12, "Bad", "Good")

# dropping variables
data <- subset(data, select = -c(G1, G2, school, Mjob, Fjob, guardian, traveltime, romantic, goout))

VariablesClasses <- data %>% summarise_all(class) %>% gather # checking variables' classes

# changing type of variables
data$Medu <- factor(data$Medu, order=T, levels = c(0,1,2,3,4))
data$Fedu <- factor(data$Fedu, order=T, levels = c(0,1,2,3,4))
data$studytime <- factor(data$studytime, order=T, levels = c(1,2,3,4))
data$famrel <- factor(data$famrel, order=T, levels = c(1,2,3,4,5))
data$freetime <- factor(data$freetime, order=T, levels = c(1,2,3,4,5))
data$Dalc <- factor(data$Dalc, order=T, levels = c(1,2,3,4,5))
data$Walc <- factor(data$Walc, order=T, levels = c(1,2,3,4,5))
data$health <- factor(data$health, order=T, levels = c(1,2,3,4,5))
data$studentPerformance <- as.factor(data$studentPerformance)

```

## Descriptive analysis

Histogram of students point in the final exam looks like a Gaussian bell curve.


```{r echo = FALSE, warning=FALSE, message=F}
summary(data)
hist(data$G3, breaks=0:20)
pie(table(data$studentPerformance)) # TODO: change to ggplot2

```


# Classification

## Model selection

## Logistic regression

In this chapter, some results concerning Logistic regression were presented.  
On the training test, the Logistic regression model achieved the success rate equal to 81.92%. However, because of frequent occurance of overfitting it's better to compare algorithms' scores on the test set. Taking into account the test set, the model predicted correctly 17 students with Bad Performance and 80 students with Good Performance. It results in the correct prediction at the level of 75.19%.
In order to determine if the obtained statistics is good or not, we compare the obtained result with the base model which is a reflection of a random guessing. Its success rate is equal to 69,77%. It follows that the logistic regression model gives us an improvement of the success rate at the level of 5.4 percentage points.
 
As can be seen on the ROC curve, there is a large advantage of the true positive rate for a threshold's range of 0.0 - 0.7. 
...
The AUC for the logistic regression amounts to 78.86%.

```{r fig.align="center", fig.width = 4, fig.height = 5 }
dataLogit <- subset(data, select = -c(G3))

set.seed(1011)
labelsLogit = sample.split(dataLogit$studentPerformance, SplitRatio = 4/5) # We split the data in the proportion 80%/20%

# checking if the split is balanced
dataLogit_train = dataLogit[labelsLogit, ]
dataLogit_trainMean <- mean(as.numeric(dataLogit_train$studentPerformance)-1)
dataLogit_test = dataLogit[!labelsLogit, ]
dataLogit_testMean <- mean(as.numeric(dataLogit_test$studentPerformance)-1)

# fit the logistic regression model on the training sample only
glm.fit = glm(studentPerformance ~ ., data = dataLogit_train, family = binomial)

# predicted probabilities on the test sample 
glm.probs = predict(glm.fit, newdata = dataLogit_test, type = "response") 

# predicted signs on the test set
glm.pred = ifelse(glm.probs > 0.5, 1, 0)

# test sample confusion matrix
LogitConfMat <- table(true = dataLogit_test$studentPerformance, predict = glm.pred)
LogitConfMat

# test sample total success rate
dataLogit_test$studentPerformance <- ifelse(dataLogit_test$studentPerformance=="Good",1,0)
LogitSuccessRate <- mean(glm.pred == dataLogit_test$studentPerformance)
LogitSuccessRate
## TODO: upewnic sie, ze 1 oznacza zawsze Good

# test set ROC curve and AUC
predob = prediction(glm.probs, dataLogit_test$studentPerformance)
perf = performance(predob, "tpr", "fpr")
plot(perf, main = "Logistic Regression", colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7))
LogitAUC <- as.numeric(performance(predob, "auc")@y.values)

# total test set success rate of the baseline model (with prediction 1 for every observation)
Basemodel <- mean(1 == dataLogit_test$studentPerformance)

# Confusion matrix and SuccessRate on training set
glm.probs_train = predict(glm.fit, newdata = dataLogit_train, type = "response") 
glm.pred_train = ifelse(glm.probs_train > 0.5, 1, 0)
LogitConfMat_train <- table(true = dataLogit_train$studentPerformance, predict = glm.pred_train)
dataLogit_train$studentPerformance <- ifelse(dataLogit_train$studentPerformance=="Good",1,0)
LogitSuccessRate_train <- mean(glm.pred_train == dataLogit_train$studentPerformance)
LogitSuccessRate_train
```

## Support Vector Machine

```{r }

dataSVM <- subset(data, select = -c(G3))

# dividing our dataset into training and test sets (80%/20%)
train = sample(nrow(dataSVM), 0.8*nrow(dataSVM))
dataSVM_train = dataSVM[train, ]
dataSVM_test = dataSVM[-train, ]

dataSVM_test.response <- dataSVM_test$studentPerformance

### 10-CV using `tune()`
set.seed(1)
# cross-validating the tuning parameters gamma and lambda for the radial SVM om training data
tune.out = tune(svm, factor(studentPerformance) ~ ., data = dataSVM_train, kernel = "radial",
                ranges = list(cost = c(1e-10, 1e-5, 0.1 ,1 ,10 ,100 ,1000), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)

# retrieve the best model
svmfit.opt = tune.out$best.model
summary(svmfit.opt)

# confusion matrix on the test set
table(true = dataSVM_test.response,  pred = predict(svmfit.opt, dataSVM_test))

SVMErrorRate <- (1 - mean(predict(svmfit.opt, dataSVM_test) == dataSVM_test.response)) # error rate

# TODO: in our dataset Number of Support Vectors is very large. How should we pick C to change it?
# TODO: how to pick kernel, cost, gamma
# TODO: add ROC curve
# TODO: parameter tuning with library(kernlab)
# TODO: why can I not pick another proportion of training/test set???
# TODO: probably because our dataset contains a lot of categorical variables, SVM method behavies very poorly. We need to verify it, and write about it in out report

```

## K-Nearest Neigbours

## Linear Discriminant Analysis

## Random Forests, Boosting, Bagging

```{r}

## RF

dataRF <- subset(data, select = -c(G3))

labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
dataRF_train = dataRF[labelsRF,]

# checking if 500 trees is enough for our dataset
checking = randomForest(studentPerformance ~ ., data = dataRF_train)
checkigPlot <- plot(checking$err.rate[,1], xlab="Number of trees", ylab="OOB error rate")  
# The OOB error rate is lowest for 500 trees used. Therefore we conclude that $B=500$ is sufficient.
```

Now we build multiple models of default random forest, test it accuracy and average the results. Accuracy of model is measured by AUC (Area Under the Curve). Random Forest uses here default tuning parameters - $500$ trees and $\lfloor sqrt(p)\rfloor = 4$ (for classification). The model is run 20 times and results averaged for better estimation.
```{r}
set.seed(17) # for replicability
auc_value20 = 0.0
repeat_each = 20
for (i in 1:repeat_each) {
  
  labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
  dataRF_train = dataRF[labelsRF,]
  dataRF_test = dataRF[!labelsRF,]
 
  randomForestResult = randomForest(studentPerformance ~ ., data = dataRF_train)
  # We use m = 4, because sqrt(23) = 4.8, p = 23

  rf.probs = predict(randomForestResult, dataRF_test, type = "prob")[, 2]
  predob.rf = prediction(rf.probs, dataRF_test$studentPerformance)
  perf = performance(predob.rf, "tpr", "fpr")
  
  auc_value <- as.numeric(performance(predob.rf, "auc")@y.values)
  auc_value20 = auc_value20 + auc_value
}

mean_auc = auc_value20 / repeat_each
mean_auc
```
The accuracy of classification by default random forest measured by AUC equal to $0.8011636$. 
 
Let's check importance of each variable with respect to predicting studentPerformance.
```{r}

# variable importance
rf = randomForest(studentPerformance ~ ., data = dataRF_train)
varImpPlot(rf) # plot importance
```
The importance of each variable is measured for each tree by the improvement in the split-criterion. Later the results for each tree are collected.
Improvement of split-criterion at each node was measured by Gini Index. 
On the plot we can see that most important predictors of students success is: number of their absences, mothers education, measure of health, reason for school choice and study time.

Now we plot ROC curve for random forest with default $mtry$ and with $mtry=p$ which is equivalent to bagging method. 

```{r }
# Plot for RF
par(mfrow = c(1, 2))
plot(perf, main = "Random Forest, m = 4", colorize = TRUE) 

# TODO: make sure if m = 23
# TODO: what about OOB?


### Bagging:
BaggingResult = randomForest(studentPerformance ~ ., data = dataRF_train, mtry = 23, ntree = 500)
bagg.probs = predict(BaggingResult, dataRF_test, type = "prob")[, 2]
predob.bagg = prediction(bagg.probs, dataRF_test$studentPerformance)
perf = performance(predob.bagg, "tpr", "fpr")
plot(perf, main = "Bagging, m = 23", colorize = TRUE)
par(mfrow = c(1, 1))
```
-
Let's now compare averaged accouracy (measured by AUC) for default random forest and bagging method. Both errors were estimated by test data, and not by OOB error. 

```{r }
auc_value20 = 0.0
repeat_each = 20
for (i in 1:repeat_each) {
  
  labelsRF = sample.split(dataRF$studentPerformance, SplitRatio = 4/5)
  dataRF_train = dataRF[labelsRF,]
  dataRF_test = dataRF[!labelsRF,]
 
  BaggingResult = randomForest(studentPerformance ~ ., data = dataRF_train, mtry = 23, ntree = 500)
  bagg.probs = predict(BaggingResult, dataRF_test, type = "prob")[, 2]
  predob.bagg = prediction(bagg.probs, dataRF_test$studentPerformance)
  
  auc_value <- as.numeric(performance(predob.bagg, "auc")@y.values)
  auc_value20 = auc_value20 + auc_value
}
mean_auc.bagging = auc_value20 / repeat_each

cat("RandomForest (m = 4): ", mean_auc)
cat("\n")
cat("Bagging      (m = 23): ", as.numeric(performance(predob.bagg, "auc")@y.values))
 
```

For random forest ($m = 4$) AUC curve is equal to $0.8011636$.
For bagging ($m = 23$) AUC curve is equal to $0.7941455$. 
 
By applying bagging method we see a slight degradation of model accuracy.
 
 
### Boosting
```{r }
set.seed(1)
dataBoosting <- subset(data, select = -c(G3))
dataBoosting$studentPerformance <- as.integer(dataBoosting$studentPerformance) 

labelsBoosting = sample.split(dataBoosting$studentPerformance, SplitRatio = 4/5)
dataBoosting_train = dataBoosting[labelsBoosting,]
dataBoosting_test = dataBoosting[!labelsBoosting,]

# We use CV to pick the number of trees.
boostingCV = gbm((studentPerformance-1) ~ ., data = dataBoosting_train, distribution = 'bernoulli', n.trees = 10000, 
                 shrinkage = 0.01, cv.folds = 10)
# Bernoulli requires the response to be in {0,1}. That's why we use (studentPerformance-1)
best.iter = gbm.perf(boostingCV, method = "cv")# Check the best iteration
# We fit the optimal boosted tree.
boostingBest = gbm((studentPerformance-1) ~ ., data = dataBoosting_train, distribution = 'bernoulli', 
                   n.trees = best.iter, shrinkage = 0.01)

# test set ROC curve and the AUC for the best boosted tree.
boost.probs = predict(boostingBest, dataBoosting_test, n.trees = best.iter, type = "response")
predob = prediction(boost.probs, dataBoosting_test$studentPerformance)
perf = performance(predob, "tpr", "fpr")
plot(perf, main = "Boosting with B = 760", colorize = TRUE) #TODO: B
as.numeric(performance(predob, "auc")@y.values)

# Variable importance plot
summary(boostingBest)

#TODO: how to pick shrinkage, d, distribution


```

For Boosting method (with $B = 760$) the AUC metric is equal to $0.7820606$.
Importance measured by boosting selected number of failures as the most relative predictor. After that, importanct variables were: mother education, number of absences, reason to choose particular school.


## Classification summary

Default random foreset AUC = 0.8011636
Random forest importance variables: absences, Medu, health, reason, studytime.

Boosting AUC = 0.7820606
Boostring importance variables: failures, Medu,, absences, reason

# Regression

## Model selection

## Linear regression

## Random Forests

## Regression summary

# Summary







